{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4adc716-9e20-4759-bb75-60391b8b7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bfc4f1-2a32-48e4-8e8b-6525181b65ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "input_data_save.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_data_raw1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_data_save.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m cap_data_raw1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcap_data_save.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m _read(fname, dtype\u001b[38;5;241m=\u001b[39mdtype, comment\u001b[38;5;241m=\u001b[39mcomment, delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[0;32m   1374\u001b[0m             converters\u001b[38;5;241m=\u001b[39mconverters, skiplines\u001b[38;5;241m=\u001b[39mskiprows, usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1375\u001b[0m             unpack\u001b[38;5;241m=\u001b[39munpack, ndmin\u001b[38;5;241m=\u001b[39mndmin, encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1376\u001b[0m             max_rows\u001b[38;5;241m=\u001b[39mmax_rows, quote\u001b[38;5;241m=\u001b[39mquotechar)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:992\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    990\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 992\u001b[0m     fh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_datasource\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mopen(path, mode, encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: input_data_save.txt not found."
     ]
    }
   ],
   "source": [
    "input_data_raw1 = np.loadtxt('input_data_save.txt')\n",
    "cap_data_raw1 = np.loadtxt('cap_data_save.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4439415-3e1b-4791-aa40-b636290901ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data_raw1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the training data to PyTorch tensors\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x_train_tensor_raw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_data_raw1, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      3\u001b[0m y_train_tensor_raw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(cap_data_raw1, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_data_raw1' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the training data to PyTorch tensors\n",
    "x_train_tensor_raw = torch.tensor(input_data_raw1, dtype=torch.float32)\n",
    "y_train_tensor_raw = torch.tensor(cap_data_raw1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9edd21-24bc-4702-9cc8-b1705f4821ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_tensor_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_tensor\u001b[39m(tensor, max_val, min_val):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (tensor \u001b[38;5;241m-\u001b[39m min_val) \u001b[38;5;241m/\u001b[39m (max_val \u001b[38;5;241m-\u001b[39m min_val)\n\u001b[1;32m----> 4\u001b[0m x_train_epsneg_raw1 \u001b[38;5;241m=\u001b[39m x_train_tensor_raw[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m x_train_epspos_raw1 \u001b[38;5;241m=\u001b[39m x_train_tensor_raw[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m max_epsneg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(x_train_epsneg_raw1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_tensor_raw' is not defined"
     ]
    }
   ],
   "source": [
    "def normalize_tensor(tensor, max_val, min_val):\n",
    "    return (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "x_train_epsneg_raw1 = x_train_tensor_raw[:, 0]\n",
    "x_train_epspos_raw1 = x_train_tensor_raw[:, 1]\n",
    "\n",
    "max_epsneg = torch.max(x_train_epsneg_raw1)\n",
    "min_epsneg = torch.min(x_train_epsneg_raw1)\n",
    "\n",
    "max_epspos = torch.max(x_train_epspos_raw1)\n",
    "min_epspos = torch.min(x_train_epspos_raw1)\n",
    "\n",
    "x_train_epsneg_norm1 = normalize_tensor(x_train_epsneg_raw1, max_epsneg, min_epsneg)\n",
    "x_train_epspos_norm1 = normalize_tensor(x_train_epspos_raw1, max_epspos, min_epspos)\n",
    "x_train = torch.stack((x_train_epsneg_norm1, x_train_epspos_norm1), dim=1)\n",
    "\n",
    "y_train_tensor = y_train_tensor_raw\n",
    "max_y = torch.max(y_train_tensor)\n",
    "min_y = torch.min(y_train_tensor)\n",
    "\n",
    "y_train_norm = normalize_tensor(y_train_tensor, max_y, min_y)\n",
    "y_train = y_train_norm.reshape(200,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbbb344-7545-4044-9549-22e14fed76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        # Define the layers\n",
    "        self.layer1 = nn.Linear(2, 15)  # Input layer (2 inputs) to first hidden layer (15 neurons)\n",
    "        self.layer2 = nn.Linear(15, 5)  # First hidden layer (15 neurons) to second hidden layer (5 neurons)\n",
    "        self.layer3 = nn.Linear(5, 1)   # Second hidden layer (5 neurons) to output layer (1 output)\n",
    "        \n",
    "        # Define activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = ANN()\n",
    "\n",
    "# Define the loss function (Mean Squared Error for regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35be9a49-d8f3-4ac9-91a7-0c1a950a281b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Forward pass: compute the model output\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(x_train)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of epochs for training\n",
    "num_epochs = 20000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: compute the model output\n",
    "    outputs = model(x_train)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass: compute the gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e1fad1-48a9-4f4c-b5f9-0f1de0adc887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: x = [0.04982824 0.45053402], Loss = 0.20632129907608032\n",
      "Step 2: x = [0.05134691 0.4509111 ], Loss = 0.20607644319534302\n",
      "Step 3: x = [0.05286559 0.4512882 ], Loss = 0.2058315873146057\n",
      "Step 4: x = [0.05438426 0.45166528], Loss = 0.2055867463350296\n",
      "Step 5: x = [0.05590294 0.45204237], Loss = 0.2053418755531311\n",
      "Step 6: x = [0.05742161 0.45241946], Loss = 0.2050970196723938\n",
      "Step 7: x = [0.05894028 0.45279655], Loss = 0.2048521637916565\n",
      "Step 8: x = [0.06045896 0.45317364], Loss = 0.2046073079109192\n",
      "Step 9: x = [0.06197763 0.45355073], Loss = 0.20436245203018188\n",
      "Step 10: x = [0.06349631 0.45392781], Loss = 0.20411759614944458\n",
      "Step 11: x = [0.06501498 0.4543049 ], Loss = 0.20387275516986847\n",
      "Step 12: x = [0.06653365 0.454682  ], Loss = 0.20362786948680878\n",
      "Step 13: x = [0.06805233 0.45505908], Loss = 0.20338302850723267\n",
      "Step 14: x = [0.069571   0.45543617], Loss = 0.20313817262649536\n",
      "Step 15: x = [0.07108968 0.45581326], Loss = 0.20289331674575806\n",
      "Step 16: x = [0.07260835 0.45619035], Loss = 0.20264846086502075\n",
      "Step 17: x = [0.07412703 0.45656744], Loss = 0.20240360498428345\n",
      "Step 18: x = [0.0756457  0.45694453], Loss = 0.20215874910354614\n",
      "Step 19: x = [0.07716437 0.4573216 ], Loss = 0.20191387832164764\n",
      "Step 20: x = [0.07868305 0.4576987 ], Loss = 0.20166902244091034\n",
      "Step 21: x = [0.08020172 0.4580758 ], Loss = 0.20142418146133423\n",
      "Step 22: x = [0.0817204  0.45845288], Loss = 0.20117932558059692\n",
      "Step 23: x = [0.08323907 0.45882997], Loss = 0.20093446969985962\n",
      "Step 24: x = [0.08475775 0.45920706], Loss = 0.20068961381912231\n",
      "Step 25: x = [0.08627642 0.45958415], Loss = 0.200444757938385\n",
      "Step 26: x = [0.08779509 0.45996124], Loss = 0.2001999020576477\n",
      "Step 27: x = [0.08931377 0.46033832], Loss = 0.1999550312757492\n",
      "Step 28: x = [0.09083244 0.4607154 ], Loss = 0.1997101753950119\n",
      "Step 29: x = [0.09235112 0.4610925 ], Loss = 0.1994653344154358\n",
      "Step 30: x = [0.09386979 0.4614696 ], Loss = 0.1992204785346985\n",
      "Step 31: x = [0.09538846 0.46184668], Loss = 0.1989756077528\n",
      "Step 32: x = [0.09690714 0.46222377], Loss = 0.19873075187206268\n",
      "Step 33: x = [0.09842581 0.46260086], Loss = 0.19848591089248657\n",
      "Step 34: x = [0.09994449 0.46297795], Loss = 0.19824105501174927\n",
      "Step 35: x = [0.10146316 0.46335503], Loss = 0.19799618422985077\n",
      "Step 36: x = [0.10298184 0.46373212], Loss = 0.19775134325027466\n",
      "Step 37: x = [0.10450051 0.4641092 ], Loss = 0.19750648736953735\n",
      "Step 38: x = [0.10601918 0.4644863 ], Loss = 0.19726161658763885\n",
      "Step 39: x = [0.10753786 0.4648634 ], Loss = 0.19701677560806274\n",
      "Step 40: x = [0.10905653 0.46524048], Loss = 0.19677191972732544\n",
      "Step 41: x = [0.11057521 0.46561757], Loss = 0.19652706384658813\n",
      "Step 42: x = [0.11209388 0.46599466], Loss = 0.19628219306468964\n",
      "Step 43: x = [0.11361255 0.46637174], Loss = 0.19603733718395233\n",
      "Step 44: x = [0.11513123 0.46674883], Loss = 0.19579248130321503\n",
      "Step 45: x = [0.1166499  0.46712592], Loss = 0.19554764032363892\n",
      "Step 46: x = [0.11816858 0.467503  ], Loss = 0.19530275464057922\n",
      "Step 47: x = [0.11968725 0.4678801 ], Loss = 0.1950579285621643\n",
      "Step 48: x = [0.12120593 0.4682572 ], Loss = 0.194813072681427\n",
      "Step 49: x = [0.1227246  0.46863428], Loss = 0.1945682018995285\n",
      "Step 50: x = [0.12424327 0.46901137], Loss = 0.19432333111763\n",
      "Step 51: x = [0.12576194 0.46938846], Loss = 0.1940784901380539\n",
      "Step 52: x = [0.12728061 0.46976554], Loss = 0.19383364915847778\n",
      "Step 53: x = [0.12879927 0.47014263], Loss = 0.19358877837657928\n",
      "Step 54: x = [0.13031794 0.47051972], Loss = 0.19334393739700317\n",
      "Step 55: x = [0.13183661 0.4708968 ], Loss = 0.19309906661510468\n",
      "Step 56: x = [0.13335527 0.4712739 ], Loss = 0.19285422563552856\n",
      "Step 57: x = [0.13487394 0.471651  ], Loss = 0.19260936975479126\n",
      "Step 58: x = [0.13639261 0.47202808], Loss = 0.19236451387405396\n",
      "Step 59: x = [0.13791128 0.47240517], Loss = 0.19211965799331665\n",
      "Step 60: x = [0.13942994 0.47278225], Loss = 0.19187480211257935\n",
      "Step 61: x = [0.14094861 0.47315934], Loss = 0.19162994623184204\n",
      "Step 62: x = [0.14246728 0.47353643], Loss = 0.19138509035110474\n",
      "Step 63: x = [0.14398594 0.47391352], Loss = 0.19114023447036743\n",
      "Step 64: x = [0.14550461 0.4742906 ], Loss = 0.19089537858963013\n",
      "Step 65: x = [0.14702328 0.4746677 ], Loss = 0.19065052270889282\n",
      "Step 66: x = [0.14854194 0.4750448 ], Loss = 0.19040566682815552\n",
      "Step 67: x = [0.15006061 0.47542188], Loss = 0.1901608109474182\n",
      "Step 68: x = [0.15157928 0.47579896], Loss = 0.1899159550666809\n",
      "Step 69: x = [0.15309794 0.47617605], Loss = 0.1896711140871048\n",
      "Step 70: x = [0.15461661 0.47655314], Loss = 0.1894262582063675\n",
      "Step 71: x = [0.15613528 0.47693023], Loss = 0.189181387424469\n",
      "Step 72: x = [0.15765394 0.47730732], Loss = 0.1889365315437317\n",
      "Step 73: x = [0.15917261 0.4776844 ], Loss = 0.18869167566299438\n",
      "Step 74: x = [0.16069128 0.4780615 ], Loss = 0.18844683468341827\n",
      "Step 75: x = [0.16220994 0.4784386 ], Loss = 0.18820197880268097\n",
      "Step 76: x = [0.16372861 0.47881567], Loss = 0.18795712292194366\n",
      "Step 77: x = [0.16524728 0.47919276], Loss = 0.18771225214004517\n",
      "Step 78: x = [0.16676594 0.47956985], Loss = 0.18746739625930786\n",
      "Step 79: x = [0.16828461 0.47994694], Loss = 0.18722257018089294\n",
      "Step 80: x = [0.16980328 0.48032403], Loss = 0.18697771430015564\n",
      "Step 81: x = [0.17132194 0.48070112], Loss = 0.18673284351825714\n",
      "Step 82: x = [0.17284061 0.4810782 ], Loss = 0.18648798763751984\n",
      "Step 83: x = [0.17435928 0.4814553 ], Loss = 0.18624313175678253\n",
      "Step 84: x = [0.17587794 0.48183239], Loss = 0.18599829077720642\n",
      "Step 85: x = [0.17739661 0.48220947], Loss = 0.18575343489646912\n",
      "Step 86: x = [0.17891528 0.48258656], Loss = 0.1855085790157318\n",
      "Step 87: x = [0.18043394 0.48296365], Loss = 0.1852637082338333\n",
      "Step 88: x = [0.18195261 0.48334074], Loss = 0.185018852353096\n",
      "Step 89: x = [0.18347128 0.48371783], Loss = 0.1847740113735199\n",
      "Step 90: x = [0.18498994 0.48409492], Loss = 0.1845291703939438\n",
      "Step 91: x = [0.18650861 0.484472  ], Loss = 0.1842842996120453\n",
      "Step 92: x = [0.18802728 0.4848491 ], Loss = 0.18403944373130798\n",
      "Step 93: x = [0.18954594 0.48522618], Loss = 0.18379458785057068\n",
      "Step 94: x = [0.19106461 0.48560327], Loss = 0.18354973196983337\n",
      "Step 95: x = [0.19258328 0.48598036], Loss = 0.18330487608909607\n",
      "Step 96: x = [0.19410194 0.48635745], Loss = 0.18306002020835876\n",
      "Step 97: x = [0.19562061 0.48673454], Loss = 0.18281516432762146\n",
      "Step 98: x = [0.19713928 0.48711163], Loss = 0.18257030844688416\n",
      "Step 99: x = [0.19865794 0.48748872], Loss = 0.18232545256614685\n",
      "Step 100: x = [0.20017661 0.4878658 ], Loss = 0.18208061158657074\n",
      "Optimized x: [0.20017661 0.4878658 ]\n",
      "Final Loss: -0.18183574080467224\n"
     ]
    }
   ],
   "source": [
    "# Define the function to optimize\n",
    "def objective_function(x):\n",
    "    tensor1 = x[0]    # 1 represents the anode porosity \n",
    "    tensor2 = x[1]    # 2 represents the cathode porosity\n",
    "    x_input = torch.stack((tensor1, tensor2), dim=0)\n",
    "    return model(x_input)\n",
    "\n",
    "# Initialize the inputs (parameters to optimize)\n",
    "val0 = torch.rand(1)    # an initial guess of anode porosity (normalized)\n",
    "val1 = torch.rand(1)    # an initial guess of cathode porosity (normalized)\n",
    "x = torch.tensor([val0, val1], requires_grad=True)\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD([x], lr=0.01)  # lr is the learning rate\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 100\n",
    "\n",
    "# Lists to store the evolution of x[0], x[1], and loss\n",
    "x0_values = []\n",
    "x1_values = []\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "    # Zero the gradients from the previous step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the objective function\n",
    "    result = objective_function(x)\n",
    "    loss = -result[0]\n",
    "\n",
    "    # Compute the gradients (backward pass)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters using the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Apply the constraints to keep x[0] and x[1] within [0, 1]\n",
    "    with torch.no_grad():\n",
    "        x.clamp_(0, 1)  # In-place operation to clamp x within [0, 1]\n",
    "\n",
    "    # Store the current values of x[0], x[1], and loss\n",
    "    x0_values.append(x[0].item())\n",
    "    x1_values.append(x[1].item())\n",
    "\n",
    "    # Print the current value of the loss and parameters\n",
    "    print(f\"Step {step + 1}: x = {x.detach().numpy()}, Loss = {loss.item()}\")\n",
    "\n",
    "# Final optimized values\n",
    "print(f\"Optimized x: {x.detach().numpy()}\")\n",
    "print(f\"Final Loss: {objective_function(x).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37586534-1161-40fa-ab4e-a2a5c8d797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the evolution of x[0] and x[1]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x0_values, label='x[0]')\n",
    "plt.plot(x1_values, label='x[1]')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Evolution of x[0] and x[1]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ad8e41-7eee-40d1-afca-1119d87ec634",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;241m*\u001b[39m (max_val \u001b[38;5;241m-\u001b[39m min_val) \u001b[38;5;241m+\u001b[39m min_val\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# de-normalize the porosity of anode and cathode\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m value_real0 \u001b[38;5;241m=\u001b[39m de_normalize_tensor(x[\u001b[38;5;241m0\u001b[39m], max_epsneg, min_epsneg)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(value_real0)    \u001b[38;5;66;03m# optimized result of anode porosity\u001b[39;00m\n\u001b[0;32m      8\u001b[0m value_real1 \u001b[38;5;241m=\u001b[39m de_normalize_tensor(x[\u001b[38;5;241m1\u001b[39m], max_epspos, min_epspos)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# define the de-normalization function\n",
    "def de_normalize_tensor(tensor, max_val, min_val):\n",
    "    return tensor * (max_val - min_val) + min_val\n",
    "    \n",
    "# de-normalize the porosity of anode and cathode\n",
    "value_real0 = de_normalize_tensor(x[0], max_epsneg, min_epsneg)\n",
    "print(value_real0)    # optimized result of anode porosity\n",
    "value_real1 = de_normalize_tensor(x[1], max_epspos, min_epspos)\n",
    "print(value_real1)    # optimized result of cathode porosity\n",
    "\n",
    "# de-normalize the capacity value (C/m^2)\n",
    "y = model(x)\n",
    "cap_optim = de_normalize_tensor(y[0], max_y, min_y)\n",
    "print(cap_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca0f11-cb7c-402a-b71b-76a3deca8d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
