{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da52b4a9-9aee-4ac6-b72c-29c2b248830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7b48c9-d85c-4bb4-aabf-2057f47d6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d82eb42-ad37-4797-b0e4-d765eeeb863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator with fewer layers and smaller output size (14x14)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e0bb8ee-5710-4210-8341-0f8fedbca2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Discriminator\n",
    "# Define the Discriminator with smaller architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff80d348-e307-45e8-914c-abd9739e21f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:04<00:00, 2.47MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 65.6kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.06MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.28MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_size = 14 * 14  # Reduced image size (14x14)\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20  # Fewer epochs for faster training\n",
    "\n",
    "# Prepare the Data with image resizing to 14x14\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(14),  # Resize images to 14x14\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48e9ea43-6acf-439c-9aa0-440660b86d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models, loss function, and optimizers\n",
    "generator = Generator(input_dim=latent_dim, output_dim=img_size)#.to('cuda')\n",
    "discriminator = Discriminator(input_dim=img_size)#.to('cuda')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b35ee22-b1b6-4a3c-8112-8992f95bc88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch 0/938                   Loss D: 0.7103700637817383, loss G: 0.7121849060058594\n",
      "Epoch [0/20] Batch 100/938                   Loss D: 0.5153858661651611, loss G: 0.7319075465202332\n",
      "Epoch [0/20] Batch 200/938                   Loss D: 0.20059561729431152, loss G: 1.4934366941452026\n",
      "Epoch [0/20] Batch 300/938                   Loss D: 0.07374270260334015, loss G: 2.4315104484558105\n",
      "Epoch [0/20] Batch 400/938                   Loss D: 0.017348507419228554, loss G: 3.8128440380096436\n",
      "Epoch [0/20] Batch 500/938                   Loss D: 0.05269809812307358, loss G: 3.093778610229492\n",
      "Epoch [0/20] Batch 600/938                   Loss D: 0.0648210421204567, loss G: 2.5211353302001953\n",
      "Epoch [0/20] Batch 700/938                   Loss D: 0.0678696408867836, loss G: 2.4523584842681885\n",
      "Epoch [0/20] Batch 800/938                   Loss D: 0.12908178567886353, loss G: 3.4602043628692627\n",
      "Epoch [0/20] Batch 900/938                   Loss D: 0.0965563952922821, loss G: 3.214473009109497\n",
      "Epoch [1/20] Batch 0/938                   Loss D: 0.0695907473564148, loss G: 3.0387625694274902\n",
      "Epoch [1/20] Batch 100/938                   Loss D: 0.14965665340423584, loss G: 1.9716906547546387\n",
      "Epoch [1/20] Batch 200/938                   Loss D: 0.09823170304298401, loss G: 2.0957155227661133\n",
      "Epoch [1/20] Batch 300/938                   Loss D: 0.6561997532844543, loss G: 1.1484062671661377\n",
      "Epoch [1/20] Batch 400/938                   Loss D: 0.2841993570327759, loss G: 1.8058351278305054\n",
      "Epoch [1/20] Batch 500/938                   Loss D: 0.4404372572898865, loss G: 1.4858914613723755\n",
      "Epoch [1/20] Batch 600/938                   Loss D: 0.24596406519412994, loss G: 1.4605258703231812\n",
      "Epoch [1/20] Batch 700/938                   Loss D: 0.16496622562408447, loss G: 2.82566499710083\n",
      "Epoch [1/20] Batch 800/938                   Loss D: 0.1814328134059906, loss G: 2.0275824069976807\n",
      "Epoch [1/20] Batch 900/938                   Loss D: 0.11067334562540054, loss G: 2.78228497505188\n",
      "Epoch [2/20] Batch 0/938                   Loss D: 0.26249998807907104, loss G: 2.10912823677063\n",
      "Epoch [2/20] Batch 100/938                   Loss D: 0.14754775166511536, loss G: 2.362257480621338\n",
      "Epoch [2/20] Batch 200/938                   Loss D: 0.11169378459453583, loss G: 3.0220489501953125\n",
      "Epoch [2/20] Batch 300/938                   Loss D: 0.26770561933517456, loss G: 1.267734169960022\n",
      "Epoch [2/20] Batch 400/938                   Loss D: 0.1617879718542099, loss G: 2.4475197792053223\n",
      "Epoch [2/20] Batch 500/938                   Loss D: 0.20032891631126404, loss G: 1.7897605895996094\n",
      "Epoch [2/20] Batch 600/938                   Loss D: 0.23376023769378662, loss G: 1.621931791305542\n",
      "Epoch [2/20] Batch 700/938                   Loss D: 0.24048754572868347, loss G: 1.634656310081482\n",
      "Epoch [2/20] Batch 800/938                   Loss D: 0.3450976610183716, loss G: 2.147346258163452\n",
      "Epoch [2/20] Batch 900/938                   Loss D: 0.196554034948349, loss G: 1.648741602897644\n",
      "Epoch [3/20] Batch 0/938                   Loss D: 0.09596289694309235, loss G: 2.536113739013672\n",
      "Epoch [3/20] Batch 100/938                   Loss D: 0.10794048011302948, loss G: 2.8704121112823486\n",
      "Epoch [3/20] Batch 200/938                   Loss D: 0.16798526048660278, loss G: 2.419971227645874\n",
      "Epoch [3/20] Batch 300/938                   Loss D: 0.27383294701576233, loss G: 2.7604923248291016\n",
      "Epoch [3/20] Batch 400/938                   Loss D: 0.07934223115444183, loss G: 3.2841055393218994\n",
      "Epoch [3/20] Batch 500/938                   Loss D: 0.24657537043094635, loss G: 2.3858554363250732\n",
      "Epoch [3/20] Batch 600/938                   Loss D: 0.17544680833816528, loss G: 2.7967655658721924\n",
      "Epoch [3/20] Batch 700/938                   Loss D: 0.2554202377796173, loss G: 2.1894845962524414\n",
      "Epoch [3/20] Batch 800/938                   Loss D: 0.10065467655658722, loss G: 2.439873218536377\n",
      "Epoch [3/20] Batch 900/938                   Loss D: 0.2680239677429199, loss G: 1.5404815673828125\n",
      "Epoch [4/20] Batch 0/938                   Loss D: 0.10237913578748703, loss G: 2.5603139400482178\n",
      "Epoch [4/20] Batch 100/938                   Loss D: 0.3025978207588196, loss G: 1.372052550315857\n",
      "Epoch [4/20] Batch 200/938                   Loss D: 0.2387564331293106, loss G: 2.011335611343384\n",
      "Epoch [4/20] Batch 300/938                   Loss D: 0.2964513301849365, loss G: 1.6648553609848022\n",
      "Epoch [4/20] Batch 400/938                   Loss D: 0.13626733422279358, loss G: 2.633190393447876\n",
      "Epoch [4/20] Batch 500/938                   Loss D: 0.1553208827972412, loss G: 3.378136157989502\n",
      "Epoch [4/20] Batch 600/938                   Loss D: 0.1464564949274063, loss G: 2.987062692642212\n",
      "Epoch [4/20] Batch 700/938                   Loss D: 0.05559777468442917, loss G: 3.760120391845703\n",
      "Epoch [4/20] Batch 800/938                   Loss D: 0.11750192195177078, loss G: 2.153491258621216\n",
      "Epoch [4/20] Batch 900/938                   Loss D: 0.07096213102340698, loss G: 3.130087375640869\n",
      "Epoch [5/20] Batch 0/938                   Loss D: 0.08833060413599014, loss G: 2.296844720840454\n",
      "Epoch [5/20] Batch 100/938                   Loss D: 0.17248372733592987, loss G: 2.417144775390625\n",
      "Epoch [5/20] Batch 200/938                   Loss D: 0.11993277817964554, loss G: 2.1429812908172607\n",
      "Epoch [5/20] Batch 300/938                   Loss D: 0.1664406657218933, loss G: 2.51008939743042\n",
      "Epoch [5/20] Batch 400/938                   Loss D: 0.13390065729618073, loss G: 3.1808977127075195\n",
      "Epoch [5/20] Batch 500/938                   Loss D: 0.09595118463039398, loss G: 3.7886414527893066\n",
      "Epoch [5/20] Batch 600/938                   Loss D: 0.2318924516439438, loss G: 2.7523505687713623\n",
      "Epoch [5/20] Batch 700/938                   Loss D: 0.14057885110378265, loss G: 2.3392140865325928\n",
      "Epoch [5/20] Batch 800/938                   Loss D: 0.1652444750070572, loss G: 2.8098411560058594\n",
      "Epoch [5/20] Batch 900/938                   Loss D: 0.1080666333436966, loss G: 2.534541606903076\n",
      "Epoch [6/20] Batch 0/938                   Loss D: 0.11984366178512573, loss G: 3.158590316772461\n",
      "Epoch [6/20] Batch 100/938                   Loss D: 0.1955603063106537, loss G: 2.3910741806030273\n",
      "Epoch [6/20] Batch 200/938                   Loss D: 0.17933066189289093, loss G: 2.700378894805908\n",
      "Epoch [6/20] Batch 300/938                   Loss D: 0.13085739314556122, loss G: 3.4442968368530273\n",
      "Epoch [6/20] Batch 400/938                   Loss D: 0.11699295043945312, loss G: 3.620744228363037\n",
      "Epoch [6/20] Batch 500/938                   Loss D: 0.1581483632326126, loss G: 3.268587112426758\n",
      "Epoch [6/20] Batch 600/938                   Loss D: 0.059334296733140945, loss G: 3.673971652984619\n",
      "Epoch [6/20] Batch 700/938                   Loss D: 0.13959407806396484, loss G: 2.656207323074341\n",
      "Epoch [6/20] Batch 800/938                   Loss D: 0.19449344277381897, loss G: 2.722784996032715\n",
      "Epoch [6/20] Batch 900/938                   Loss D: 0.20152850449085236, loss G: 1.9306960105895996\n",
      "Epoch [7/20] Batch 0/938                   Loss D: 0.1810910999774933, loss G: 3.5287184715270996\n",
      "Epoch [7/20] Batch 100/938                   Loss D: 0.07732205092906952, loss G: 3.199913740158081\n",
      "Epoch [7/20] Batch 200/938                   Loss D: 0.2107459306716919, loss G: 2.77776837348938\n",
      "Epoch [7/20] Batch 300/938                   Loss D: 0.14786945283412933, loss G: 2.7457077503204346\n",
      "Epoch [7/20] Batch 400/938                   Loss D: 0.1405247002840042, loss G: 3.628612995147705\n",
      "Epoch [7/20] Batch 500/938                   Loss D: 0.22485782206058502, loss G: 3.2895376682281494\n",
      "Epoch [7/20] Batch 600/938                   Loss D: 0.19586801528930664, loss G: 3.748394727706909\n",
      "Epoch [7/20] Batch 700/938                   Loss D: 0.34977924823760986, loss G: 2.3378801345825195\n",
      "Epoch [7/20] Batch 800/938                   Loss D: 0.08535637706518173, loss G: 3.1951558589935303\n",
      "Epoch [7/20] Batch 900/938                   Loss D: 0.11285170912742615, loss G: 2.7544829845428467\n",
      "Epoch [8/20] Batch 0/938                   Loss D: 0.19073380529880524, loss G: 2.4451444149017334\n",
      "Epoch [8/20] Batch 100/938                   Loss D: 0.15424087643623352, loss G: 2.531951427459717\n",
      "Epoch [8/20] Batch 200/938                   Loss D: 0.23583997786045074, loss G: 3.147820234298706\n",
      "Epoch [8/20] Batch 300/938                   Loss D: 0.10583800822496414, loss G: 2.722627639770508\n",
      "Epoch [8/20] Batch 400/938                   Loss D: 0.2941688895225525, loss G: 2.336392402648926\n",
      "Epoch [8/20] Batch 500/938                   Loss D: 0.31226640939712524, loss G: 2.298919916152954\n",
      "Epoch [8/20] Batch 600/938                   Loss D: 0.17309457063674927, loss G: 2.214592218399048\n",
      "Epoch [8/20] Batch 700/938                   Loss D: 0.20162536203861237, loss G: 2.486867904663086\n",
      "Epoch [8/20] Batch 800/938                   Loss D: 0.22758954763412476, loss G: 2.4051105976104736\n",
      "Epoch [8/20] Batch 900/938                   Loss D: 0.14062343537807465, loss G: 2.5899016857147217\n",
      "Epoch [9/20] Batch 0/938                   Loss D: 0.11901926249265671, loss G: 4.0065999031066895\n",
      "Epoch [9/20] Batch 100/938                   Loss D: 0.263152152299881, loss G: 3.621607780456543\n",
      "Epoch [9/20] Batch 200/938                   Loss D: 0.12212356179952621, loss G: 3.2583868503570557\n",
      "Epoch [9/20] Batch 300/938                   Loss D: 0.11470513045787811, loss G: 2.8859403133392334\n",
      "Epoch [9/20] Batch 400/938                   Loss D: 0.31173446774482727, loss G: 3.4912383556365967\n",
      "Epoch [9/20] Batch 500/938                   Loss D: 0.22624239325523376, loss G: 3.370535373687744\n",
      "Epoch [9/20] Batch 600/938                   Loss D: 0.2181723415851593, loss G: 2.2651517391204834\n",
      "Epoch [9/20] Batch 700/938                   Loss D: 0.16456148028373718, loss G: 3.169079303741455\n",
      "Epoch [9/20] Batch 800/938                   Loss D: 0.272701233625412, loss G: 3.218785285949707\n",
      "Epoch [9/20] Batch 900/938                   Loss D: 0.3773217499256134, loss G: 2.3745908737182617\n",
      "Epoch [10/20] Batch 0/938                   Loss D: 0.35414403676986694, loss G: 3.195094108581543\n",
      "Epoch [10/20] Batch 100/938                   Loss D: 0.1594436764717102, loss G: 2.359072208404541\n",
      "Epoch [10/20] Batch 200/938                   Loss D: 0.16654923558235168, loss G: 2.878129243850708\n",
      "Epoch [10/20] Batch 300/938                   Loss D: 0.18236514925956726, loss G: 3.1106185913085938\n",
      "Epoch [10/20] Batch 400/938                   Loss D: 0.2195175588130951, loss G: 1.986175537109375\n",
      "Epoch [10/20] Batch 500/938                   Loss D: 0.13108529150485992, loss G: 3.148533582687378\n",
      "Epoch [10/20] Batch 600/938                   Loss D: 0.1316947638988495, loss G: 2.633596897125244\n",
      "Epoch [10/20] Batch 700/938                   Loss D: 0.20614874362945557, loss G: 2.077796220779419\n",
      "Epoch [10/20] Batch 800/938                   Loss D: 0.25621581077575684, loss G: 3.1242756843566895\n",
      "Epoch [10/20] Batch 900/938                   Loss D: 0.31343019008636475, loss G: 2.5251102447509766\n",
      "Epoch [11/20] Batch 0/938                   Loss D: 0.3155168294906616, loss G: 1.8500597476959229\n",
      "Epoch [11/20] Batch 100/938                   Loss D: 0.2785848081111908, loss G: 3.479968547821045\n",
      "Epoch [11/20] Batch 200/938                   Loss D: 0.14245343208312988, loss G: 3.1640126705169678\n",
      "Epoch [11/20] Batch 300/938                   Loss D: 0.15646086633205414, loss G: 3.5575942993164062\n",
      "Epoch [11/20] Batch 400/938                   Loss D: 0.17097201943397522, loss G: 2.7782201766967773\n",
      "Epoch [11/20] Batch 500/938                   Loss D: 0.3246416449546814, loss G: 2.3104560375213623\n",
      "Epoch [11/20] Batch 600/938                   Loss D: 0.1967363953590393, loss G: 3.2068593502044678\n",
      "Epoch [11/20] Batch 700/938                   Loss D: 0.1447409689426422, loss G: 2.890584707260132\n",
      "Epoch [11/20] Batch 800/938                   Loss D: 0.1493094265460968, loss G: 3.4314591884613037\n",
      "Epoch [11/20] Batch 900/938                   Loss D: 0.1261099874973297, loss G: 2.756340742111206\n",
      "Epoch [12/20] Batch 0/938                   Loss D: 0.1863909512758255, loss G: 3.080315351486206\n",
      "Epoch [12/20] Batch 100/938                   Loss D: 0.2877688705921173, loss G: 2.944310426712036\n",
      "Epoch [12/20] Batch 200/938                   Loss D: 0.36154091358184814, loss G: 2.2645204067230225\n",
      "Epoch [12/20] Batch 300/938                   Loss D: 0.19371622800827026, loss G: 2.383589029312134\n",
      "Epoch [12/20] Batch 400/938                   Loss D: 0.24640078842639923, loss G: 2.8306050300598145\n",
      "Epoch [12/20] Batch 500/938                   Loss D: 0.0962660163640976, loss G: 2.8873813152313232\n",
      "Epoch [12/20] Batch 600/938                   Loss D: 0.20242518186569214, loss G: 2.666903495788574\n",
      "Epoch [12/20] Batch 700/938                   Loss D: 0.27859264612197876, loss G: 2.6002931594848633\n",
      "Epoch [12/20] Batch 800/938                   Loss D: 0.38925886154174805, loss G: 2.0287818908691406\n",
      "Epoch [12/20] Batch 900/938                   Loss D: 0.17961230874061584, loss G: 2.1068263053894043\n",
      "Epoch [13/20] Batch 0/938                   Loss D: 0.2382250875234604, loss G: 2.6290228366851807\n",
      "Epoch [13/20] Batch 100/938                   Loss D: 0.1961466372013092, loss G: 2.5952796936035156\n",
      "Epoch [13/20] Batch 200/938                   Loss D: 0.4699859619140625, loss G: 2.1582932472229004\n",
      "Epoch [13/20] Batch 300/938                   Loss D: 0.3673884868621826, loss G: 1.6029611825942993\n",
      "Epoch [13/20] Batch 400/938                   Loss D: 0.24322432279586792, loss G: 2.4790704250335693\n",
      "Epoch [13/20] Batch 500/938                   Loss D: 0.26038599014282227, loss G: 2.5944809913635254\n",
      "Epoch [13/20] Batch 600/938                   Loss D: 0.2538345754146576, loss G: 2.7481536865234375\n",
      "Epoch [13/20] Batch 700/938                   Loss D: 0.47131219506263733, loss G: 2.2574820518493652\n",
      "Epoch [13/20] Batch 800/938                   Loss D: 0.32646340131759644, loss G: 2.2555928230285645\n",
      "Epoch [13/20] Batch 900/938                   Loss D: 0.3407469391822815, loss G: 1.7293330430984497\n",
      "Epoch [14/20] Batch 0/938                   Loss D: 0.36384788155555725, loss G: 2.5566961765289307\n",
      "Epoch [14/20] Batch 100/938                   Loss D: 0.3863511383533478, loss G: 2.1928017139434814\n",
      "Epoch [14/20] Batch 200/938                   Loss D: 0.3393770754337311, loss G: 2.714460611343384\n",
      "Epoch [14/20] Batch 300/938                   Loss D: 0.34684157371520996, loss G: 1.492958426475525\n",
      "Epoch [14/20] Batch 400/938                   Loss D: 0.30580660700798035, loss G: 2.0891518592834473\n",
      "Epoch [14/20] Batch 500/938                   Loss D: 0.36855655908584595, loss G: 1.6022226810455322\n",
      "Epoch [14/20] Batch 600/938                   Loss D: 0.4615960717201233, loss G: 1.5048980712890625\n",
      "Epoch [14/20] Batch 700/938                   Loss D: 0.4791201949119568, loss G: 2.0186991691589355\n",
      "Epoch [14/20] Batch 800/938                   Loss D: 0.37329623103141785, loss G: 1.44646155834198\n",
      "Epoch [14/20] Batch 900/938                   Loss D: 0.4517671465873718, loss G: 1.473024845123291\n",
      "Epoch [15/20] Batch 0/938                   Loss D: 0.507109522819519, loss G: 1.6254794597625732\n",
      "Epoch [15/20] Batch 100/938                   Loss D: 0.3376001715660095, loss G: 2.4293007850646973\n",
      "Epoch [15/20] Batch 200/938                   Loss D: 0.4953128397464752, loss G: 1.8920972347259521\n",
      "Epoch [15/20] Batch 300/938                   Loss D: 0.4757377505302429, loss G: 1.4661531448364258\n",
      "Epoch [15/20] Batch 400/938                   Loss D: 0.30450713634490967, loss G: 1.8850727081298828\n",
      "Epoch [15/20] Batch 500/938                   Loss D: 0.5662675499916077, loss G: 1.480345606803894\n",
      "Epoch [15/20] Batch 600/938                   Loss D: 0.48361748456954956, loss G: 1.924490213394165\n",
      "Epoch [15/20] Batch 700/938                   Loss D: 0.3730330467224121, loss G: 1.7123055458068848\n",
      "Epoch [15/20] Batch 800/938                   Loss D: 0.3473125398159027, loss G: 1.7303450107574463\n",
      "Epoch [15/20] Batch 900/938                   Loss D: 0.3578413128852844, loss G: 1.7869497537612915\n",
      "Epoch [16/20] Batch 0/938                   Loss D: 0.2693668305873871, loss G: 2.211529493331909\n",
      "Epoch [16/20] Batch 100/938                   Loss D: 0.3085862994194031, loss G: 1.8944635391235352\n",
      "Epoch [16/20] Batch 200/938                   Loss D: 0.2554396986961365, loss G: 2.3617758750915527\n",
      "Epoch [16/20] Batch 300/938                   Loss D: 0.3920237123966217, loss G: 2.7142603397369385\n",
      "Epoch [16/20] Batch 400/938                   Loss D: 0.4867244362831116, loss G: 1.9451606273651123\n",
      "Epoch [16/20] Batch 500/938                   Loss D: 0.28188079595565796, loss G: 2.2373220920562744\n",
      "Epoch [16/20] Batch 600/938                   Loss D: 0.24026885628700256, loss G: 2.1452229022979736\n",
      "Epoch [16/20] Batch 700/938                   Loss D: 0.30747929215431213, loss G: 1.7449144124984741\n",
      "Epoch [16/20] Batch 800/938                   Loss D: 0.3742440938949585, loss G: 2.4456441402435303\n",
      "Epoch [16/20] Batch 900/938                   Loss D: 0.2753828167915344, loss G: 2.3786652088165283\n",
      "Epoch [17/20] Batch 0/938                   Loss D: 0.4707484841346741, loss G: 1.3651098012924194\n",
      "Epoch [17/20] Batch 100/938                   Loss D: 0.3453947603702545, loss G: 1.8877992630004883\n",
      "Epoch [17/20] Batch 200/938                   Loss D: 0.2839365005493164, loss G: 2.081505537033081\n",
      "Epoch [17/20] Batch 300/938                   Loss D: 0.2537286877632141, loss G: 2.379202127456665\n",
      "Epoch [17/20] Batch 400/938                   Loss D: 0.5454728603363037, loss G: 2.033118486404419\n",
      "Epoch [17/20] Batch 500/938                   Loss D: 0.32906973361968994, loss G: 1.7880580425262451\n",
      "Epoch [17/20] Batch 600/938                   Loss D: 0.5028858184814453, loss G: 1.829333782196045\n",
      "Epoch [17/20] Batch 700/938                   Loss D: 0.33873283863067627, loss G: 1.8540836572647095\n",
      "Epoch [17/20] Batch 800/938                   Loss D: 0.2706349492073059, loss G: 1.8758379220962524\n",
      "Epoch [17/20] Batch 900/938                   Loss D: 0.28036606311798096, loss G: 2.0461106300354004\n",
      "Epoch [18/20] Batch 0/938                   Loss D: 0.31396740674972534, loss G: 2.5747435092926025\n",
      "Epoch [18/20] Batch 100/938                   Loss D: 0.2000538557767868, loss G: 2.18445086479187\n",
      "Epoch [18/20] Batch 200/938                   Loss D: 0.4148327112197876, loss G: 1.8477383852005005\n",
      "Epoch [18/20] Batch 300/938                   Loss D: 0.3009653389453888, loss G: 1.9292956590652466\n",
      "Epoch [18/20] Batch 400/938                   Loss D: 0.39073121547698975, loss G: 1.9722949266433716\n",
      "Epoch [18/20] Batch 500/938                   Loss D: 0.43570762872695923, loss G: 1.7183374166488647\n",
      "Epoch [18/20] Batch 600/938                   Loss D: 0.2735283374786377, loss G: 2.493018627166748\n",
      "Epoch [18/20] Batch 700/938                   Loss D: 0.2737879753112793, loss G: 2.197687864303589\n",
      "Epoch [18/20] Batch 800/938                   Loss D: 0.26591598987579346, loss G: 2.094459295272827\n",
      "Epoch [18/20] Batch 900/938                   Loss D: 0.32694220542907715, loss G: 2.4377894401550293\n",
      "Epoch [19/20] Batch 0/938                   Loss D: 0.36247557401657104, loss G: 1.8757256269454956\n",
      "Epoch [19/20] Batch 100/938                   Loss D: 0.3734414279460907, loss G: 1.5535919666290283\n",
      "Epoch [19/20] Batch 200/938                   Loss D: 0.2799164056777954, loss G: 2.0184710025787354\n",
      "Epoch [19/20] Batch 300/938                   Loss D: 0.3595240116119385, loss G: 1.8263909816741943\n",
      "Epoch [19/20] Batch 400/938                   Loss D: 0.38265201449394226, loss G: 1.5521565675735474\n",
      "Epoch [19/20] Batch 500/938                   Loss D: 0.2474868893623352, loss G: 2.002264976501465\n",
      "Epoch [19/20] Batch 600/938                   Loss D: 0.3585222661495209, loss G: 2.160118341445923\n",
      "Epoch [19/20] Batch 700/938                   Loss D: 0.2781744599342346, loss G: 1.9245117902755737\n",
      "Epoch [19/20] Batch 800/938                   Loss D: 0.6152969598770142, loss G: 2.587270498275757\n",
      "Epoch [19/20] Batch 900/938                   Loss D: 0.39627236127853394, loss G: 1.8649816513061523\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((imgs.size(0), 1), requires_grad=False)#.to('cuda')\n",
    "        fake = torch.zeros((imgs.size(0), 1), requires_grad=False)#.to('cuda')\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = imgs.view(imgs.size(0), -1)#.to('cuda')\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.randn((imgs.size(0), latent_dim))#.to('cuda')\n",
    "        gen_imgs = generator(z)\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {i}/{len(train_loader)} \\\n",
    "                  Loss D: {d_loss.item()}, loss G: {g_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598f9ef-4d1c-423d-b3c4-979eac3c9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Generated and Original Images\n",
    "def show_images(gen_images, real_images):\n",
    "    gen_images = gen_images.view(gen_images.size(0), 1, 14, 14).cpu().data\n",
    "    real_images = real_images.view(real_images.size(0), 1, 14, 14).cpu().data\n",
    "    \n",
    "    # Concatenate generated and real images\n",
    "    images = torch.cat([gen_images, real_images])\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow=8, normalize=True)\n",
    "    \n",
    "    # Set smaller figure size\n",
    "    plt.figure(figsize=(6, 6))  # Smaller figure\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.title('Top Half: Generated Images, Bottom Half: Original Images')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate images\n",
    "z = torch.randn(16, latent_dim)#.to('cuda')  # Reduced to 16 for a smaller plot\n",
    "gen_imgs = generator(z)\n",
    "\n",
    "# Get a batch of real images\n",
    "real_imgs, _ = next(iter(train_loader))\n",
    "real_imgs = real_imgs[:16].view(16, -1)#.to('cuda')  # Reduced to 16 for a smaller plot\n",
    "\n",
    "# Show generated vs original images\n",
    "show_images(gen_imgs, real_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a837368-79ba-4bb5-be50-006eb2798556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
